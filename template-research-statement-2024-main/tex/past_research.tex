


 \subsection{\textbf{Past Research}}
 I elected to delve deep into the field of perception with my master's thesis. The thesis investigated the impact of pretraining via Masked Image Modeling, with a particular focus on the Masked Autoencoding \cite{mae} technique, for improving the downstream task of object detection. In order to achieve real-time performance within a flexible detection framework we chose CenterNet, reframing object detection as a keypoint estimation. By working on 2D object detection on monocular images, and 3D object detection on monocular images and on bird's eye view representations of LiDAR point clouds, I was able to gain a comprehensive overview of the field of object detection. 
 The practical challenge of training a vision transformer on a small dataset, with large image size, and with a model with little inductive biases compared to CNN-based models, became apparent. To mitigate these challenges, I concentrated my efforts on the larger Waymo Open Perception dataset, I used bird's eye view representations of the lidar point clouds. Moreover, I used the hierarchical vision transformer Hiera, which incorporates additional inductive biases, such as local attention in early layers and token pooling.
 In \autoref{fig:mae_img} an exemplary MAE reconstruction of a bird's eye view point cloud representation on the Waymo Open Perception dataset is presented. I confirmed that the MAE pretext task is beneficial for the downstream task of 3D object detection, evaluated on the Waymo Open Perception validation set.

I continued my research at the Research Center for Information Technology (FZI), where I designed data and training pipelines to incorporate video sequence input as well as multimodal inputs into the hierarchical transformer. The multimodal approach to fuse bird's eye view and spherical view of the lidar point cloud is illustrated in \autoref{fig:fusion}. A fused representation is obtained by jointly masking and reconstructing the embeddings of the two modalities. Finally by benchmarking against a ResNet backbone we showed that in the low data regime CNN-based architectures are still highly competitive in object detection. The opportunity to train on the JÃ¼lich Supercomputing Centre, has enhanced my professional approach to machine learning at FZI.

In a second project at FZI, I processed global point clouds and global annotations from KITTI-360, converting them into a format suitable for frame-by-frame object detection, similar to the KITTI dataset. The project demonstrates that a theoretically optimal frame-by-frame object detector still requires optimal dissection and accumulation of the point clouds and annotations in order to perform well on the KITTI-360 object detection benchmark.

 





