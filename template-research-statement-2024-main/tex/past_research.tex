


 \subsubsection{Past research}

This led to my master's thesis titled "Masked Autoencoding as Pre-Training for Traffic Participant Detection," 
reviewed by Prof. Dr.-Ing. Christoph Stiller.
The goal of the thesis was to investigate self-supervised learning as a pretext task for downstream 2D and 3D object detection in the context 
of autonomous driving. For pretraining Masked Image Modeling in particular the popular Masked Autoencoding \cite{mae} technique was used. To achieve real-time performance within a flexible detection framework we chose CenterNet. It reframes 
object detection as a keypoint estimation task and additionally regress bounding box parameters from the predicted keypoints.
By performing 2D object detection on monocular images, 3D object detection on monocular images, and 3D object detection on bird's eye view
representations of the  lidar point cloud I was able to get an overview of the object detection field.
First we benchmarked the detector on KITTI using a ResNet backbone. After replacing the ResNet with a vision transformer we met 
the practical challenge of small dataset size, large image size, and model size when training a vision transformer with little 
inductive biases compared to CNN-based models.
While I saw decent results when using a pretrained vision transformer in the detectron-2 framework,
training a vision transformer from scratch on KITTI is not practical.
To mitigate these challenges we focussed on the larger Waymo Open Perception dataset using 
bird's eye view representations of the lidar point cloud. Further we investigated the impact of other vision transformers and found 
the hierarchical vision transformer Hiera \cite{hiera}. It comes with inductive biases like local attention in early layers and 
token pooling. Through the clever application of mask units Hiera allows for 
the use of Masked Autoencoding (MAE) as a pretext task. In \autoref{fig:mae_img} a exemplary MAE reconstruction of a 
bird's eye view point cloud representation
on the Waymo Open Perception dataset is shown. We showed that the 
MAE pretext task is beneficial for the downstream task of 3D object detection evaluated on the Waymo Open Perception validation set. 
regarding augmentation our findings align with 
prior research namely that pointcloud rotation augmentation increases the performance and mirror augmentation decreases the performance.

I continued my research at the Research Center for Information Technology (FZI), where I designed data and training pipelines 
to incorporate video sequence input as well as multimodal inputs to the Hiera model. The multimodal approach to fuse bird's eye view and 
spherical view of the lidar point cloud as shown in \autoref{fig:fusion} by jointly masking 
and reconstructing the embeddings of the two views. For that I designed separate encoders for the modalities 
similar to the \cite{multimae}. During MAE pretraining both modalities are reconstructed and during downstream 3D object detection
only the bird's eye view was used for the detection task. Ultimately we confirmed that the MAE task is beneficial 
for the downstream task of 3D object detection. Furthermore MAE can be used to fuse multimodal inputs in a self-supervised way.
Finally by benchmarking against a ResNet backbone we showed that in the low data regime CNN-based architectures 
are still highly competitive in object detection.
The opportunity to utilize large public like Waymo Open Perception in addition 
 to training on the JÃ¼lich Supercomputing Centre, has enhanced my professional approach to machine learning. 
In a second project at FZI I processed accumulated point clouds and annotations in KITTI-360 
 towards a format suitable for frame by frame object detection similar to the KITTI dataset. The project shows
 that a theoretically optimal frame by frame object detector still needs optimal dissection and 
 accumulation of the point clouds and annotations to perform well on the KITTI-360 object detection benchmark.
 





