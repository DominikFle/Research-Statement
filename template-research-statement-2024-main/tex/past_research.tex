


 \subsection{\textbf{Past Research}}
 I chose to delve deep into the field of perception with my master's thesis. During the thesis I investigated the impact of pretraining via Masked Image Modeling in particular the Masked Autoencoding \cite{mae} technique on improving the downstream task of object detection. To achieve real-time performance within a flexible detection framework we chose CenterNet, reframing object detection as a keypoint estimation. By working on 2D object detection on monocular images, and 3D object detection on monocular images and on bird's eye view representations of LiDAR point clouds, I gained a comprehensive overview of the field of object detection. 
 Using vision transformer I was confronted with the practical challenge of small dataset size, large image size, and model size when training a vision transformer with little inductive biases compared to CNN-based models. To mitigate these challenges we focused on the larger Waymo Open Perception dataset using bird's eye view representations of the lidar point clouds. Furthermore I used the hierachical vision transformer which comes with additional inductive biases like local attention in early layers and token pooling. In \autoref{fig:mae_img} a exemplary MAE reconstruction of a bird's eye view point cloud representation on the Waymo Open Perception dataset is shown. We confirmed that the MAE pretext task is beneficial for the downstream task of 3D object detection evaluated on the Waymo Open Perception validation set.

 I continued my research at the Research Center for Information Technology (FZI), where I designed data and training pipelines to incorporate video sequence input as well as multimodal inputs tot he hierarchical transformer. The multimodal approach to fuse bird's eye view and spherical view of the lidar point cloud is shown in \autoref{fig:fusion}. A fused representation is obtained by jointly masking and reconstructing the embeddings of the two modalities. Ultimately we confirmed that the MAE task is beneficial for the downstream task of 3D object detection. Finally by benchmarking against a ResNet backbone we showed that in the low data regime CNN-based architectures are still highly competitive in object detection. At FZI the opportunity to train on the JÃ¼lich Supercomputing Centre, has enhanced my professional approach to machine learning.

 In a second project at FZI, I processed global point clouds and global annotations from KITTI-360, converting them into a format suitable for frame-by-frame object detection, similar to the KITTI dataset. The project shows that a theoretically optimal frame-by-frame object detector still needs optimal dissection and accumulation of the point clouds and annotations to perform well on the KITTI-360 object detection benchmark.
 





